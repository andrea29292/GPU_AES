\chapter{Parallelizzazione AES in CUDA}
Per poter ottenere buoni risultati nel campo del GPU Computing c'è bisogno di un algoritmo che ci consenta un sufficiente grado di parallelizzazione.
La scelta di AES infatti non è casuale: basti pensare a come l'algoritmo opera \textbf{indipendentemente} su uno stato o su un altro senza concatenare le due cose e subito ci accorgiamo che consente un alto grado di \textbf{parallelizzazione}.
L'obiettivo del nostro progetto è quindi quello di dimostrare come l'alto grado di parallelizzazione offerta dall'architettura CUDA possa essere utile nell'algoritmo AES. 

\paragraph{Implementazione in C}In una prima fase abbiamo sviluppato l'algoritmo in C per poterlo usare poi come confronto con la versione in CUDA. Sia la versione C che la versione CUDA seguono le seguenti fasi:
\begin{itemize}
\item Lettura del \textbf{plainText}, il testo in chiaro scritto in esadecimale
\item Lettura di una chiave da 16 caratteri esadecimali e generazione di una chiave \textbf{estesa} da 176 sufficente per 11 round
\item Separazione del plainText in stati da 16 elementi.
\item Esecuzione dell'algoritmo su ogni stato
\item Controllo della corretta esecuzione tramite confronto con vettore di test
\item Salvataggio dei risultati per fattore tempo
\end{itemize}

Per quanto riguarda i risultati in fattore tempo questi vengono calcolati comprendendo non solo la fase vera e propria dell'esecuzione di AES ma anche quella di separazione degli stati visto che nel caso GPU questa comprende anche il passaggio dei dati da \textbf{host memory} (memoria centrale) a \textbf{GPU memory}. Abbiamo voluto includere nelle prestazioni questa fase considerando che la memoria è un noto \textit{bottleneck} da considerare.

\section{Prima implementazione in CUDA - Un solo stato}
Una prima implementazione dell'algoritmo in CUDA non considerava una pluralità di stati ma si concentrava solo sulla parallelizzazione delle \textbf{funzioni interne} quali subBytes, addRoundKey ecc...
Purtroppo i risultati furono scoraggianti, infatti nonostante fossimo riusciti a replicare il funzionamento di AES in CUDA verificandone il successo con il vettore di test, i tempi erano di molto peggiori rispetto alla versione C.
\paragraph{Perchè?} Abbiamo pensato che un solo stato non fosse sufficiente per evidenziare un miglioramento nella nostra versione e che questo fosse dovuto proprio a un bottleneck della memoria.

\section{Seconda implementazione in CUDA - Più stati}
Sulla considerazione precedente abbiamo applicato l'algoritmo a più stati contemporaneamente criptando porzioni di testo maggiori e non più limitate a 16 caratteri esadecimali.
In questo modo non solo abbiamo aumentato le dimensioni dei test ma abbiamo anche ottenuto una parallelizzazione più "esterna", in aggiunta a quella già implementata.

Vediamo un esempio di come abbiamo parallelizzato le funzioni sugli stati utilizzate da AES.

\begin{lstlisting}[caption={Versione sequenziale di addRoundKey}]
void addRoundKey(uChar** state, int cur_round) {
	for (int i = 0; i < columns; i++) {
		for (int j = 0; j < rows; j++) {
			int val = (cur_round * 16) + ((i * 4) + j);
			state[j][i] = state[j][i] ^ expanded_key[val];

		}
	}
	
	\end{lstlisting}

\begin{lstlisting}[caption={Versione parallela di addRoundKey}]
__device__ void addRoundKey(uChar* state, int cur_round, uChar* expanded_keyGPU) {
	int Row = blockIdx.y * blockDim.y + threadIdx.y;
	int Col = blockIdx.x * blockDim.x + threadIdx.x;
	int index = Col * M + Row;
	int val = (cur_round * 16) + index;
	state[index] = state[index] ^ expanded_keyGPU[val];
}
\end{lstlisting}

Nella versione sequenziale lo xor tra stato e chiave espansa viene effettuato con due cicli for con un costo di \(O(n)\) dove \(n\) è la dimensione dello stato quindi 16. Nella versione sequenziale, sfruttando i thread arriviamo invece ad un \(O(1)\)

Purtroppo neanche questa versione è risultata conveniente dando risultati peggiori della versione C.

\section{Ultima implementazione}
Utilizzando Nsight ci siamo resi conto dell'alto numero di kernel che lanciavamo per ogni test considerando che ad ogni stato veniva lanciato un kernel e che eseguivamo test da un minimo di 15 mila stati circa fino a un massimo di 2 milioni circa. L'idea è stata quella di utilizzare un solo kernel con un sufficiente numero di thread da servire tutti gli stati.
Questa soluzione si è poi rivelata quella vincente consentendoci di arrivare a un tempo più o meno \textbf{costante} di esecuzione e superando di parecchio la versione C, come vedremo nel prossimo capitolo.